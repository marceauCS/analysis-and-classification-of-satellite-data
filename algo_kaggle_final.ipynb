{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importer les bases de données train et test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5}\n",
    "\n",
    "## Read csvs\n",
    "\n",
    "train = gpd.read_file('./train.geojson', index_col=0)\n",
    "test = gpd.read_file('./test.geojson', index_col=0)\n",
    "train_y = train['change_type'].apply(lambda x: change_type_map[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "travail sur le feature geometry, création de nouveaux features plus facile pour les calculs après : périmètre, surface, disance moyenne des points au centroid, variance des distances des points au centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, Point\n",
    "import numpy as np\n",
    "\n",
    "def calculate_area(polygon):\n",
    "    area = polygon.area\n",
    "    return area\n",
    "\n",
    "def calculate_perimeter(polygon):\n",
    "    perimeter = polygon.length\n",
    "    return perimeter\n",
    "\n",
    "def calculate_var_distance_to_center(polygon):\n",
    "    center = polygon.centroid\n",
    "    exterior_coords = polygon.exterior.coords #on enlève la dernière coordonnée puisque c'est le meme point que le premier\n",
    "    distances = [center.distance(Point(x, y)) for x, y in exterior_coords]\n",
    "    return np.var(distances)\n",
    "\n",
    "def calculate_mean_distance(polygon):\n",
    "    center = polygon.centroid\n",
    "    exterior_coords = polygon.exterior.coords\n",
    "    distances = [center.distance(Point(x, y)) for x, y in exterior_coords]\n",
    "    return np.mean(distances)\n",
    "\n",
    "train['area'] = train['geometry'].apply(calculate_area)\n",
    "train['perimeter'] = train['geometry'].apply(calculate_perimeter)\n",
    "train['variance_distances_to_center'] = train['geometry'].apply(calculate_var_distance_to_center)\n",
    "train['calculate_mean_distance'] = train['geometry'].apply(calculate_mean_distance)\n",
    "train.drop(columns = ['geometry'])\n",
    "\n",
    "test['area'] = test['geometry'].apply(calculate_area)\n",
    "test['perimeter'] = test['geometry'].apply(calculate_perimeter)\n",
    "test['variance_distances_to_center'] = test['geometry'].apply(calculate_var_distance_to_center)\n",
    "test['calculate_mean_distance'] = test['geometry'].apply(calculate_mean_distance)\n",
    "test.drop(columns = ['geometry'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait le hot one encoding sur les features 'urban_types' et 'geography_types'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_types = ['Dense Urban', 'Sparse Urban', 'N,A', 'Urban Slum', 'Rural', 'Industrial']\n",
    "for urban_type in urban_types:\n",
    "    #si l'urban type est présent dans la colonne urban_types on ajoute 1 à la colonne urban_type (qui a été ajoutée au df)\n",
    "    train[urban_type]=train['urban_type'].apply(lambda x: urban_type in x).astype(int) \n",
    "    test[urban_type]=test['urban_type'].apply(lambda x: urban_type in x).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geography_types = ['Hills', 'Farms', 'Barren Land', 'Dense Forest', 'N,A', 'Coastal', 'Lakes', 'Desert', 'Sparse Forest', 'River', 'Grass Land', 'Snow']\n",
    "for geography_type in geography_types:\n",
    "    train[geography_type]=train['geography_type'].apply(lambda x: geography_type in x).astype(int) \n",
    "    test[geography_type]=test['geography_type'].apply(lambda x: geography_type in x).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On repasse tout dans l'ordre chronologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for i in range (5):\n",
    "    train[f'date{i}']=pd.to_datetime(train[f'date{i}'])\n",
    "    test[f'date{i}']=pd.to_datetime(test[f'date{i}'])\n",
    "    \n",
    "train_sort_by_date=train[['date0','date1','date2','date3','date4']]\n",
    "test_sort_by_date=test[['date0','date1','date2','date3','date4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for idx in range(5):\n",
    "    train[f'date{idx}'] = pd.to_datetime(train[f'date{idx}'])\n",
    "\n",
    "dates_df = train[[f'date{i}' for i in range(5)]]\n",
    "dates_df['dates_list'] = dates_df.apply(lambda x: [x[f'date{i}'] for i in range(5)], axis=1)\n",
    "dates_df['sorted_indices'] = dates_df['dates_list'].apply(lambda x: np.argsort(x))\n",
    "dates_df['dates_list'] = dates_df['dates_list'].apply(lambda x: [x[i] for i in np.argsort(x)])\n",
    "\n",
    "change_status_df = train[[f'change_status_date{i}' for i in range(5)]]\n",
    "dates_df['sorted_change_status'] = change_status_df.apply(lambda x: np.array([x[f'change_status_date{i}'] for i in range(5)]), axis=1)\n",
    "dates_df['sorted_change_status'] = dates_df.apply(lambda x: x['sorted_change_status'][x['sorted_indices']], axis=1)\n",
    "\n",
    "# Sort image mean values for green and red channels\n",
    "green_mean_df = train[[f'img_green_mean_date{i}' for i in range(1, 6)]]\n",
    "red_mean_df = train[[f'img_red_mean_date{i}' for i in range(1, 6)]]\n",
    "dates_df['sorted_green_mean'] = green_mean_df.apply(lambda x: np.array([x[f'img_green_mean_date{i}'] for i in range(1, 6)]), axis=1)\n",
    "dates_df['sorted_red_mean'] = red_mean_df.apply(lambda x: np.array([x[f'img_red_mean_date{i}'] for i in range(1, 6)]), axis=1)\n",
    "dates_df['sorted_green_mean'] = dates_df.apply(lambda x: x['sorted_green_mean'][x['sorted_indices']], axis=1)\n",
    "dates_df['sorted_red_mean'] = dates_df.apply(lambda x: x['sorted_red_mean'][x['sorted_indices']], axis=1)\n",
    "\n",
    "# Update the original DataFrame with sorted data\n",
    "for i in range(5):\n",
    "    train[f'date{i}'] = dates_df['dates_list'].apply(lambda x: x[i])\n",
    "    train[f'change_status_date{i}'] = dates_df['sorted_change_status'].apply(lambda x: x[i])\n",
    "    train[f'img_green_mean_date{i+1}'] = dates_df['sorted_green_mean'].apply(lambda x: x[i])\n",
    "    train[f'img_red_mean_date{i+1}'] = dates_df['sorted_red_mean'].apply(lambda x: x[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for idx in range(5):\n",
    "    test[f'date{idx}'] = pd.to_datetime(test[f'date{idx}'])\n",
    "\n",
    "dates_test_df = test[[f'date{i}' for i in range(5)]]\n",
    "dates_test_df['dates_list'] = dates_test_df.apply(lambda x: [x[f'date{i}'] for i in range(5)], axis=1)\n",
    "dates_test_df['sorted_indices'] = dates_test_df['dates_list'].apply(lambda x: np.argsort(x))\n",
    "dates_test_df['dates_list'] = dates_test_df['dates_list'].apply(lambda x: [x[i] for i in np.argsort(x)])\n",
    "\n",
    "change_status_test_df = test[[f'change_status_date{i}' for i in range(5)]]\n",
    "dates_test_df['sorted_change_status'] = change_status_test_df.apply(lambda x: np.array([x[f'change_status_date{i}'] for i in range(5)]), axis=1)\n",
    "dates_test_df['sorted_change_status'] = dates_test_df.apply(lambda x: x['sorted_change_status'][x['sorted_indices']], axis=1)\n",
    "\n",
    "# Sort image mean values for green and red channels\n",
    "green_mean_test_df = test[[f'img_green_mean_date{i}' for i in range(1, 6)]]\n",
    "red_mean_test_df = test[[f'img_red_mean_date{i}' for i in range(1, 6)]]\n",
    "dates_test_df['sorted_green_mean'] = green_mean_test_df.apply(lambda x: np.array([x[f'img_green_mean_date{i}'] for i in range(1, 6)]), axis=1)\n",
    "dates_test_df['sorted_red_mean'] = red_mean_test_df.apply(lambda x: np.array([x[f'img_red_mean_date{i}'] for i in range(1, 6)]), axis=1)\n",
    "dates_test_df['sorted_green_mean'] = dates_test_df.apply(lambda x: x['sorted_green_mean'][x['sorted_indices']], axis=1)\n",
    "dates_test_df['sorted_red_mean'] = dates_test_df.apply(lambda x: x['sorted_red_mean'][x['sorted_indices']], axis=1)\n",
    "\n",
    "# Update the original DataFrame with sorted data\n",
    "for i in range(5):\n",
    "    test[f'date{i}'] = dates_test_df['dates_list'].apply(lambda x: x[i])\n",
    "    test[f'change_status_date{i}'] = dates_test_df['sorted_change_status'].apply(lambda x: x[i])\n",
    "    test[f'img_green_mean_date{i+1}'] = dates_test_df['sorted_green_mean'].apply(lambda x: x[i])\n",
    "    test[f'img_red_mean_date{i+1}'] = dates_test_df['sorted_red_mean'].apply(lambda x: x[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les dataframe avec les dates et les statuts dans le bon ordre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dummy_columns(df, date_index):\n",
    "    dummy_df = pd.get_dummies(df[f'change_status_date{date_index}'])\n",
    "    columns = [f'{column}_date{date_index}' for column in dummy_df.columns]\n",
    "    dummy_df.columns = columns\n",
    "    return dummy_df\n",
    "\n",
    "dummy_columns = [create_dummy_columns(train, i) for i in range(5)]\n",
    "\n",
    "train_change_status = pd.concat(dummy_columns, axis=1)\n",
    "\n",
    "train_df = pd.concat([train, train_change_status], axis=1)\n",
    "\n",
    "dummy_columns_test = [create_dummy_columns(test, i) for i in range(5)]\n",
    "\n",
    "test_change_status = pd.concat(dummy_columns_test, axis=1)\n",
    "\n",
    "test_df = pd.concat([test, test_change_status], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée les nouvaux features qui calculent le temps passé entre les différentes photos prises d'un même environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['passed_time_0']=(train_df['date1']-train_df['date0']).apply(lambda x: x.total_seconds())\n",
    "train_df['passed_time_1']=(train_df['date2']-train_df['date1']).apply(lambda x: x.total_seconds())\n",
    "train_df['passed_time_2']=(train_df['date3']-train_df['date2']).apply(lambda x: x.total_seconds())\n",
    "train_df['passed_time_3']=(train_df['date4']-train_df['date3']).apply(lambda x: x.total_seconds())\n",
    "\n",
    "test_df['passed_time_0']=(test_df['date1']-test_df['date0']).apply(lambda x: x.total_seconds())\n",
    "test_df['passed_time_1']=(test_df['date2']-test_df['date1']).apply(lambda x: x.total_seconds())\n",
    "test_df['passed_time_2']=(test_df['date3']-test_df['date2']).apply(lambda x: x.total_seconds())\n",
    "test_df['passed_time_3']=(test_df['date4']-test_df['date3']).apply(lambda x: x.total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les fichiers train et test qui sont prêts à servir de base de données pour créer des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_df,train_y],axis=1).to_csv('train_processed.csv')\n",
    "pd.concat([test_df],axis=1).to_csv('test_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge ces fichiers, on met les NaN à 0 par défaut et on fait un XGBoost avec les paramètres qui fonctionnent le mieux pour nous, on enregistre le CSV de submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Charger les données\n",
    "data = pd.read_csv('train_processed.csv', index_col=0)\n",
    "data = data.fillna(0)\n",
    "X = data.iloc[:, :-1].drop(columns=['N,A', 'Snow', 'urban_type', 'geography_type', 'change_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4','change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3' ,'change_status_date4'])\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "data_test = pd.read_csv('test_processed.csv', index_col=0)\n",
    "data_test = data_test.fillna(0)\n",
    "X_test_p = data_test.iloc[:, :].drop(columns=['N,A', 'Snow', 'urban_type', 'geography_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4','change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3' ,'change_status_date4'])\n",
    "\n",
    "# Définir les paramètres du modèle XGBoost\n",
    "xgb_params = {\n",
    "    'max_depth': 150,        # Profondeur maximale de l'arbre\n",
    "    'learning_rate': 0.1,   # Taux d'apprentissage\n",
    "    'n_estimators': 1000,    # Nombre d'arbres à entraîner\n",
    "    'objective': 'multi:softmax',  # Fonction d'objectif pour une classification multiclasse\n",
    "    'num_class': 6,         # Nombre de classes dans les données\n",
    "    'eval_metric': 'mlogloss'     # Métrique d'évaluation\n",
    "}\n",
    "\n",
    "# Initialiser et entraîner le modèle XGBoost\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "xgb.fit(X,Y)\n",
    "\n",
    "y_test = xgb.predict(X_test_p)\n",
    "pred_df = pd.DataFrame(y_test, columns=['change_type'])\n",
    "pred_df.to_csv(\"submission_LMTPE.csv\", index=True, index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
