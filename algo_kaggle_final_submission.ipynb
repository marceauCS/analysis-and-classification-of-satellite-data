{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer les bases de donn√©es train et test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  return ogr_read(\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5}\n",
    "\n",
    "## Read csvs\n",
    "train = gpd.read_file('./train.geojson', index_col=0)\n",
    "test = gpd.read_file('./test.geojson', index_col=0)\n",
    "train_y = train['change_type'].apply(lambda x: change_type_map[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis and extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on the geometry feature, create new features that are easier to use in later computations:\n",
    "\n",
    "perimeter, area, average distance of points to the centroid, variance of the distances of points to the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urban_type</th>\n",
       "      <th>geography_type</th>\n",
       "      <th>img_red_mean_date1</th>\n",
       "      <th>img_green_mean_date1</th>\n",
       "      <th>img_blue_mean_date1</th>\n",
       "      <th>img_red_std_date1</th>\n",
       "      <th>img_green_std_date1</th>\n",
       "      <th>img_blue_std_date1</th>\n",
       "      <th>img_red_mean_date2</th>\n",
       "      <th>img_green_mean_date2</th>\n",
       "      <th>...</th>\n",
       "      <th>change_status_date2</th>\n",
       "      <th>date3</th>\n",
       "      <th>change_status_date3</th>\n",
       "      <th>date4</th>\n",
       "      <th>change_status_date4</th>\n",
       "      <th>index</th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>variance_distances_to_center</th>\n",
       "      <th>calculate_mean_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N,A</td>\n",
       "      <td>Barren Land</td>\n",
       "      <td>203.333944</td>\n",
       "      <td>169.801486</td>\n",
       "      <td>152.200651</td>\n",
       "      <td>15.566236</td>\n",
       "      <td>16.008455</td>\n",
       "      <td>17.248278</td>\n",
       "      <td>187.051282</td>\n",
       "      <td>158.300672</td>\n",
       "      <td>...</td>\n",
       "      <td>Materials Introduced</td>\n",
       "      <td>18-07-2019</td>\n",
       "      <td>Materials Introduced</td>\n",
       "      <td>20-02-2018</td>\n",
       "      <td>Materials Introduced</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017644e-07</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>4.182693e-11</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rural</td>\n",
       "      <td>Desert</td>\n",
       "      <td>172.061525</td>\n",
       "      <td>147.685474</td>\n",
       "      <td>133.694478</td>\n",
       "      <td>41.475540</td>\n",
       "      <td>44.100325</td>\n",
       "      <td>45.832053</td>\n",
       "      <td>155.780912</td>\n",
       "      <td>135.136855</td>\n",
       "      <td>...</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>18-07-2019</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>20-02-2018</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>1</td>\n",
       "      <td>3.359779e-08</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>1.575673e-11</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rural</td>\n",
       "      <td>Desert</td>\n",
       "      <td>159.368826</td>\n",
       "      <td>142.102024</td>\n",
       "      <td>127.979757</td>\n",
       "      <td>25.094292</td>\n",
       "      <td>22.485283</td>\n",
       "      <td>23.314198</td>\n",
       "      <td>160.848178</td>\n",
       "      <td>145.442915</td>\n",
       "      <td>...</td>\n",
       "      <td>Construction Midway</td>\n",
       "      <td>18-07-2019</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>20-02-2018</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>2</td>\n",
       "      <td>2.582968e-08</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>2.345599e-12</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N,A</td>\n",
       "      <td>Desert</td>\n",
       "      <td>200.918653</td>\n",
       "      <td>176.145032</td>\n",
       "      <td>160.827045</td>\n",
       "      <td>17.500086</td>\n",
       "      <td>19.342180</td>\n",
       "      <td>20.587617</td>\n",
       "      <td>175.678701</td>\n",
       "      <td>169.652776</td>\n",
       "      <td>...</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>18-07-2019</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>20-02-2018</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>3</td>\n",
       "      <td>4.355159e-07</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>1.664449e-09</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dense Urban</td>\n",
       "      <td>Barren Land,Sparse Forest</td>\n",
       "      <td>166.621344</td>\n",
       "      <td>154.779757</td>\n",
       "      <td>149.726308</td>\n",
       "      <td>68.843646</td>\n",
       "      <td>72.361309</td>\n",
       "      <td>71.799610</td>\n",
       "      <td>149.589170</td>\n",
       "      <td>143.869542</td>\n",
       "      <td>...</td>\n",
       "      <td>Construction Midway</td>\n",
       "      <td>18-07-2019</td>\n",
       "      <td>Land Cleared</td>\n",
       "      <td>20-02-2018</td>\n",
       "      <td>Construction Midway</td>\n",
       "      <td>4</td>\n",
       "      <td>9.817937e-08</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>7.654623e-09</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120521</th>\n",
       "      <td>N,A</td>\n",
       "      <td>Sparse Forest,Grass Land,Lakes</td>\n",
       "      <td>74.594406</td>\n",
       "      <td>72.981199</td>\n",
       "      <td>62.822552</td>\n",
       "      <td>24.472312</td>\n",
       "      <td>19.897044</td>\n",
       "      <td>19.887582</td>\n",
       "      <td>89.628466</td>\n",
       "      <td>85.693444</td>\n",
       "      <td>...</td>\n",
       "      <td>Land Cleared</td>\n",
       "      <td>19-12-2013</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>29-07-2019</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>120521</td>\n",
       "      <td>3.984273e-07</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>1.065996e-09</td>\n",
       "      <td>0.000621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120522</th>\n",
       "      <td>N,A</td>\n",
       "      <td>Sparse Forest,Grass Land,Lakes</td>\n",
       "      <td>58.439701</td>\n",
       "      <td>62.549749</td>\n",
       "      <td>55.780079</td>\n",
       "      <td>21.440513</td>\n",
       "      <td>17.952821</td>\n",
       "      <td>17.289851</td>\n",
       "      <td>78.664141</td>\n",
       "      <td>73.863068</td>\n",
       "      <td>...</td>\n",
       "      <td>Land Cleared</td>\n",
       "      <td>19-12-2013</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>29-07-2019</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>120522</td>\n",
       "      <td>3.472850e-07</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>4.592515e-10</td>\n",
       "      <td>0.000539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120523</th>\n",
       "      <td>N,A</td>\n",
       "      <td>Sparse Forest,Grass Land,Lakes</td>\n",
       "      <td>42.641541</td>\n",
       "      <td>50.166332</td>\n",
       "      <td>49.730848</td>\n",
       "      <td>10.641932</td>\n",
       "      <td>9.309863</td>\n",
       "      <td>6.805194</td>\n",
       "      <td>52.940873</td>\n",
       "      <td>60.286320</td>\n",
       "      <td>...</td>\n",
       "      <td>Land Cleared</td>\n",
       "      <td>19-12-2013</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>29-07-2019</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>120523</td>\n",
       "      <td>3.052487e-07</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>1.198213e-10</td>\n",
       "      <td>0.000482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120524</th>\n",
       "      <td>Sparse Urban</td>\n",
       "      <td>Coastal,Dense Forest,Grass Land</td>\n",
       "      <td>47.581117</td>\n",
       "      <td>55.697246</td>\n",
       "      <td>49.512326</td>\n",
       "      <td>10.637095</td>\n",
       "      <td>9.854314</td>\n",
       "      <td>7.726375</td>\n",
       "      <td>64.082919</td>\n",
       "      <td>68.935690</td>\n",
       "      <td>...</td>\n",
       "      <td>Land Cleared</td>\n",
       "      <td>19-12-2013</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>29-07-2019</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>120524</td>\n",
       "      <td>2.733826e-07</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>2.131814e-10</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120525</th>\n",
       "      <td>Sparse Urban</td>\n",
       "      <td>Coastal,Dense Forest,Grass Land</td>\n",
       "      <td>45.844264</td>\n",
       "      <td>50.861506</td>\n",
       "      <td>46.086066</td>\n",
       "      <td>17.589026</td>\n",
       "      <td>15.504034</td>\n",
       "      <td>12.487233</td>\n",
       "      <td>54.559190</td>\n",
       "      <td>64.704079</td>\n",
       "      <td>...</td>\n",
       "      <td>Land Cleared</td>\n",
       "      <td>19-12-2013</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>29-07-2019</td>\n",
       "      <td>Construction Done</td>\n",
       "      <td>120525</td>\n",
       "      <td>2.422171e-07</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>2.521251e-11</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120526 rows √ó 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          urban_type                   geography_type  img_red_mean_date1  \\\n",
       "0                N,A                      Barren Land          203.333944   \n",
       "1              Rural                           Desert          172.061525   \n",
       "2              Rural                           Desert          159.368826   \n",
       "3                N,A                           Desert          200.918653   \n",
       "4        Dense Urban        Barren Land,Sparse Forest          166.621344   \n",
       "...              ...                              ...                 ...   \n",
       "120521           N,A   Sparse Forest,Grass Land,Lakes           74.594406   \n",
       "120522           N,A   Sparse Forest,Grass Land,Lakes           58.439701   \n",
       "120523           N,A   Sparse Forest,Grass Land,Lakes           42.641541   \n",
       "120524  Sparse Urban  Coastal,Dense Forest,Grass Land           47.581117   \n",
       "120525  Sparse Urban  Coastal,Dense Forest,Grass Land           45.844264   \n",
       "\n",
       "        img_green_mean_date1  img_blue_mean_date1  img_red_std_date1  \\\n",
       "0                 169.801486           152.200651          15.566236   \n",
       "1                 147.685474           133.694478          41.475540   \n",
       "2                 142.102024           127.979757          25.094292   \n",
       "3                 176.145032           160.827045          17.500086   \n",
       "4                 154.779757           149.726308          68.843646   \n",
       "...                      ...                  ...                ...   \n",
       "120521             72.981199            62.822552          24.472312   \n",
       "120522             62.549749            55.780079          21.440513   \n",
       "120523             50.166332            49.730848          10.641932   \n",
       "120524             55.697246            49.512326          10.637095   \n",
       "120525             50.861506            46.086066          17.589026   \n",
       "\n",
       "        img_green_std_date1  img_blue_std_date1  img_red_mean_date2  \\\n",
       "0                 16.008455           17.248278          187.051282   \n",
       "1                 44.100325           45.832053          155.780912   \n",
       "2                 22.485283           23.314198          160.848178   \n",
       "3                 19.342180           20.587617          175.678701   \n",
       "4                 72.361309           71.799610          149.589170   \n",
       "...                     ...                 ...                 ...   \n",
       "120521            19.897044           19.887582           89.628466   \n",
       "120522            17.952821           17.289851           78.664141   \n",
       "120523             9.309863            6.805194           52.940873   \n",
       "120524             9.854314            7.726375           64.082919   \n",
       "120525            15.504034           12.487233           54.559190   \n",
       "\n",
       "        img_green_mean_date2  ...   change_status_date2       date3  \\\n",
       "0                 158.300672  ...  Materials Introduced  18-07-2019   \n",
       "1                 135.136855  ...     Construction Done  18-07-2019   \n",
       "2                 145.442915  ...   Construction Midway  18-07-2019   \n",
       "3                 169.652776  ...     Construction Done  18-07-2019   \n",
       "4                 143.869542  ...   Construction Midway  18-07-2019   \n",
       "...                      ...  ...                   ...         ...   \n",
       "120521             85.693444  ...          Land Cleared  19-12-2013   \n",
       "120522             73.863068  ...          Land Cleared  19-12-2013   \n",
       "120523             60.286320  ...          Land Cleared  19-12-2013   \n",
       "120524             68.935690  ...          Land Cleared  19-12-2013   \n",
       "120525             64.704079  ...          Land Cleared  19-12-2013   \n",
       "\n",
       "         change_status_date3       date4   change_status_date4   index  \\\n",
       "0       Materials Introduced  20-02-2018  Materials Introduced       0   \n",
       "1          Construction Done  20-02-2018     Construction Done       1   \n",
       "2          Construction Done  20-02-2018     Construction Done       2   \n",
       "3          Construction Done  20-02-2018     Construction Done       3   \n",
       "4               Land Cleared  20-02-2018   Construction Midway       4   \n",
       "...                      ...         ...                   ...     ...   \n",
       "120521             Greenland  29-07-2019     Construction Done  120521   \n",
       "120522             Greenland  29-07-2019     Construction Done  120522   \n",
       "120523             Greenland  29-07-2019     Construction Done  120523   \n",
       "120524             Greenland  29-07-2019     Construction Done  120524   \n",
       "120525             Greenland  29-07-2019     Construction Done  120525   \n",
       "\n",
       "                area  perimeter  variance_distances_to_center  \\\n",
       "0       1.017644e-07   0.001293                  4.182693e-11   \n",
       "1       3.359779e-08   0.000735                  1.575673e-11   \n",
       "2       2.582968e-08   0.000649                  2.345599e-12   \n",
       "3       4.355159e-07   0.002690                  1.664449e-09   \n",
       "4       9.817937e-08   0.001898                  7.654623e-09   \n",
       "...              ...        ...                           ...   \n",
       "120521  3.984273e-07   0.003063                  1.065996e-09   \n",
       "120522  3.472850e-07   0.002739                  4.592515e-10   \n",
       "120523  3.052487e-07   0.002492                  1.198213e-10   \n",
       "120524  2.733826e-07   0.002335                  2.131814e-10   \n",
       "120525  2.422171e-07   0.002132                  2.521251e-11   \n",
       "\n",
       "        calculate_mean_distance  \n",
       "0                      0.000232  \n",
       "1                      0.000131  \n",
       "2                      0.000116  \n",
       "3                      0.000488  \n",
       "4                      0.000290  \n",
       "...                         ...  \n",
       "120521                 0.000621  \n",
       "120522                 0.000539  \n",
       "120523                 0.000482  \n",
       "120524                 0.000447  \n",
       "120525                 0.000403  \n",
       "\n",
       "[120526 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_area(polygon):\n",
    "    area = polygon.area\n",
    "    return area\n",
    "\n",
    "def calculate_perimeter(polygon):\n",
    "    perimeter = polygon.length\n",
    "    return perimeter\n",
    "\n",
    "def calculate_var_distance_to_center(polygon):\n",
    "    center = polygon.centroid\n",
    "    exterior_coords = polygon.exterior.coords #on enl√®ve la derni√®re coordonn√©e puisque c'est le meme point que le premier\n",
    "    distances = [center.distance(Point(x, y)) for x, y in exterior_coords]\n",
    "    return np.var(distances)\n",
    "\n",
    "def calculate_mean_distance(polygon):\n",
    "    center = polygon.centroid\n",
    "    exterior_coords = polygon.exterior.coords\n",
    "    distances = [center.distance(Point(x, y)) for x, y in exterior_coords]\n",
    "    return np.mean(distances)\n",
    "\n",
    "train['area'] = train['geometry'].apply(calculate_area)\n",
    "train['perimeter'] = train['geometry'].apply(calculate_perimeter)\n",
    "train['variance_distances_to_center'] = train['geometry'].apply(calculate_var_distance_to_center)\n",
    "train['calculate_mean_distance'] = train['geometry'].apply(calculate_mean_distance)\n",
    "train.drop(columns = ['geometry'])\n",
    "\n",
    "test['area'] = test['geometry'].apply(calculate_area)\n",
    "test['perimeter'] = test['geometry'].apply(calculate_perimeter)\n",
    "test['variance_distances_to_center'] = test['geometry'].apply(calculate_var_distance_to_center)\n",
    "test['calculate_mean_distance'] = test['geometry'].apply(calculate_mean_distance)\n",
    "test.drop(columns = ['geometry'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform One-Hot encoding on the features 'urban_types' and 'geography_types'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_types = ['Dense Urban', 'Sparse Urban', 'N,A', 'Urban Slum', 'Rural', 'Industrial']\n",
    "for urban_type in urban_types:\n",
    "    #si l'urban type est pr√©sent dans la colonne urban_types on ajoute 1 √† la colonne urban_type (qui a √©t√© ajout√©e au df)\n",
    "    train[urban_type]=train['urban_type'].apply(lambda x: urban_type in x).astype(int) \n",
    "    test[urban_type]=test['urban_type'].apply(lambda x: urban_type in x).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "geography_types = ['Hills', 'Farms', 'Barren Land', 'Dense Forest', 'N,A', 'Coastal', 'Lakes', 'Desert', 'Sparse Forest', 'River', 'Grass Land', 'Snow']\n",
    "for geography_type in geography_types:\n",
    "    train[geography_type]=train['geography_type'].apply(lambda x: geography_type in x).astype(int) \n",
    "    test[geography_type]=test['geography_type'].apply(lambda x: geography_type in x).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on the date format and sort them chronologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    train[f'date{i}'] = pd.to_datetime(train[f'date{i}'], dayfirst=True, errors='coerce')\n",
    "    test[f'date{i}'] = pd.to_datetime(test[f'date{i}'], dayfirst=True, errors='coerce')\n",
    "\n",
    "    \n",
    "train_sort_by_date=train[['date0','date1','date2','date3','date4']]\n",
    "test_sort_by_date=test[['date0','date1','date2','date3','date4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes the five timestamped observations (date0‚Äìdate4) and their associated features in the training dataset, reorders the corresponding change_status values and image channel means (green and red) accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    train[f'date{idx}'] = pd.to_datetime(train[f'date{idx}'])\n",
    "\n",
    "dates_df = train[[f'date{i}' for i in range(5)]]\n",
    "dates_df['dates_list'] = dates_df.apply(lambda x: [x[f'date{i}'] for i in range(5)], axis=1)\n",
    "dates_df['sorted_indices'] = dates_df['dates_list'].apply(lambda x: np.argsort(x))\n",
    "dates_df['dates_list'] = dates_df['dates_list'].apply(lambda x: [x[i] for i in np.argsort(x)])\n",
    "\n",
    "change_status_df = train[[f'change_status_date{i}' for i in range(5)]]\n",
    "dates_df['sorted_change_status'] = change_status_df.apply(lambda x: np.array([x[f'change_status_date{i}'] for i in range(5)]), axis=1)\n",
    "dates_df['sorted_change_status'] = dates_df.apply(lambda x: x['sorted_change_status'][x['sorted_indices']], axis=1)\n",
    "\n",
    "# Sort image mean values for green and red channels\n",
    "green_mean_df = train[[f'img_green_mean_date{i}' for i in range(1, 6)]]\n",
    "red_mean_df = train[[f'img_red_mean_date{i}' for i in range(1, 6)]]\n",
    "dates_df['sorted_green_mean'] = green_mean_df.apply(lambda x: np.array([x[f'img_green_mean_date{i}'] for i in range(1, 6)]), axis=1)\n",
    "dates_df['sorted_red_mean'] = red_mean_df.apply(lambda x: np.array([x[f'img_red_mean_date{i}'] for i in range(1, 6)]), axis=1)\n",
    "dates_df['sorted_green_mean'] = dates_df.apply(lambda x: x['sorted_green_mean'][x['sorted_indices']], axis=1)\n",
    "dates_df['sorted_red_mean'] = dates_df.apply(lambda x: x['sorted_red_mean'][x['sorted_indices']], axis=1)\n",
    "\n",
    "# Update the original DataFrame with sorted data\n",
    "for i in range(5):\n",
    "    train[f'date{i}'] = dates_df['dates_list'].apply(lambda x: x[i])\n",
    "    train[f'change_status_date{i}'] = dates_df['sorted_change_status'].apply(lambda x: x[i])\n",
    "    train[f'img_green_mean_date{i+1}'] = dates_df['sorted_green_mean'].apply(lambda x: x[i])\n",
    "    train[f'img_red_mean_date{i+1}'] = dates_df['sorted_red_mean'].apply(lambda x: x[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    test[f'date{idx}'] = pd.to_datetime(test[f'date{idx}'])\n",
    "\n",
    "dates_test_df = test[[f'date{i}' for i in range(5)]]\n",
    "dates_test_df['dates_list'] = dates_test_df.apply(lambda x: [x[f'date{i}'] for i in range(5)], axis=1)\n",
    "dates_test_df['sorted_indices'] = dates_test_df['dates_list'].apply(lambda x: np.argsort(x))\n",
    "dates_test_df['dates_list'] = dates_test_df['dates_list'].apply(lambda x: [x[i] for i in np.argsort(x)])\n",
    "\n",
    "change_status_test_df = test[[f'change_status_date{i}' for i in range(5)]]\n",
    "dates_test_df['sorted_change_status'] = change_status_test_df.apply(lambda x: np.array([x[f'change_status_date{i}'] for i in range(5)]), axis=1)\n",
    "dates_test_df['sorted_change_status'] = dates_test_df.apply(lambda x: x['sorted_change_status'][x['sorted_indices']], axis=1)\n",
    "\n",
    "# Sort image mean values for green and red channels\n",
    "green_mean_test_df = test[[f'img_green_mean_date{i}' for i in range(1, 6)]]\n",
    "red_mean_test_df = test[[f'img_red_mean_date{i}' for i in range(1, 6)]]\n",
    "dates_test_df['sorted_green_mean'] = green_mean_test_df.apply(lambda x: np.array([x[f'img_green_mean_date{i}'] for i in range(1, 6)]), axis=1)\n",
    "dates_test_df['sorted_red_mean'] = red_mean_test_df.apply(lambda x: np.array([x[f'img_red_mean_date{i}'] for i in range(1, 6)]), axis=1)\n",
    "dates_test_df['sorted_green_mean'] = dates_test_df.apply(lambda x: x['sorted_green_mean'][x['sorted_indices']], axis=1)\n",
    "dates_test_df['sorted_red_mean'] = dates_test_df.apply(lambda x: x['sorted_red_mean'][x['sorted_indices']], axis=1)\n",
    "\n",
    "# Update the original DataFrame with sorted data\n",
    "for i in range(5):\n",
    "    test[f'date{i}'] = dates_test_df['dates_list'].apply(lambda x: x[i])\n",
    "    test[f'change_status_date{i}'] = dates_test_df['sorted_change_status'].apply(lambda x: x[i])\n",
    "    test[f'img_green_mean_date{i+1}'] = dates_test_df['sorted_green_mean'].apply(lambda x: x[i])\n",
    "    test[f'img_red_mean_date{i+1}'] = dates_test_df['sorted_red_mean'].apply(lambda x: x[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally arrange the features and creates a function that detects changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_columns(df, date_index):\n",
    "    dummy_df = pd.get_dummies(df[f'change_status_date{date_index}'])\n",
    "    columns = [f'{column}_date{date_index}' for column in dummy_df.columns]\n",
    "    dummy_df.columns = columns\n",
    "    return dummy_df\n",
    "\n",
    "dummy_columns = [create_dummy_columns(train, i) for i in range(5)]\n",
    "\n",
    "train_change_status = pd.concat(dummy_columns, axis=1)\n",
    "\n",
    "train_df = pd.concat([train, train_change_status], axis=1)\n",
    "\n",
    "dummy_columns_test = [create_dummy_columns(test, i) for i in range(5)]\n",
    "\n",
    "test_change_status = pd.concat(dummy_columns_test, axis=1)\n",
    "\n",
    "test_df = pd.concat([test, test_change_status], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of new features that calculate the time intervals between photos taken in the same environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['passed_time_0']=(train_df['date1']-train_df['date0']).apply(lambda x: x.total_seconds())\n",
    "train_df['passed_time_1']=(train_df['date2']-train_df['date1']).apply(lambda x: x.total_seconds())\n",
    "train_df['passed_time_2']=(train_df['date3']-train_df['date2']).apply(lambda x: x.total_seconds())\n",
    "train_df['passed_time_3']=(train_df['date4']-train_df['date3']).apply(lambda x: x.total_seconds())\n",
    "\n",
    "test_df['passed_time_0']=(test_df['date1']-test_df['date0']).apply(lambda x: x.total_seconds())\n",
    "test_df['passed_time_1']=(test_df['date2']-test_df['date1']).apply(lambda x: x.total_seconds())\n",
    "test_df['passed_time_2']=(test_df['date3']-test_df['date2']).apply(lambda x: x.total_seconds())\n",
    "test_df['passed_time_3']=(test_df['date4']-test_df['date3']).apply(lambda x: x.total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final processed data saved to perform ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_df,train_y],axis=1).to_csv('train_processed.csv')\n",
    "pd.concat([test_df],axis=1).to_csv('test_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge ces fichiers, on met les NaN √† 0 par d√©faut et on fait un XGBoost avec les param√®tres qui fonctionnent le mieux pour nous, on enregistre le CSV de submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "data = pd.read_csv('train_processed.csv', index_col=0)\n",
    "data = data.fillna(0)\n",
    "X = data.iloc[:, :-1].drop(columns=['N,A', 'Snow', 'urban_type', 'geography_type', 'change_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4','change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3' ,'change_status_date4'])\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "data_test = pd.read_csv('test_processed.csv', index_col=0)\n",
    "data_test = data_test.fillna(0)\n",
    "X_test_p = data_test.iloc[:, :].drop(columns=['N,A', 'Snow', 'urban_type', 'geography_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4','change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3' ,'change_status_date4'])\n",
    "\n",
    "# D√©finir les param√®tres du mod√®le XGBoost\n",
    "xgb_params = {\n",
    "    'max_depth': 150,        # Profondeur maximale de l'arbre\n",
    "    'learning_rate': 0.1,   # Taux d'apprentissage\n",
    "    'n_estimators': 1000,    # Nombre d'arbres √† entra√Æner\n",
    "    'objective': 'multi:softmax',  # Fonction d'objectif pour une classification multiclasse\n",
    "    'num_class': 6,         # Nombre de classes dans les donn√©es\n",
    "    'eval_metric': 'mlogloss'     # M√©trique d'√©valuation\n",
    "}\n",
    "\n",
    "# Initialiser et entra√Æner le mod√®le XGBoost\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "xgb.fit(X,Y)\n",
    "\n",
    "y_test = xgb.predict(X_test_p)\n",
    "pred_df = pd.DataFrame(y_test, columns=['change_type'])\n",
    "pred_df.to_csv(\"submission_LMTPE.csv\", index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and Kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "data = pd.read_csv('train_processed.csv', index_col=0)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Features (X) et target (Y)\n",
    "X = data.iloc[:, :-1].drop(columns=['N,A', 'Snow', 'urban_type', 'geography_type', 'change_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4','change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3' ,'change_status_date4'])\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "# Split train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=Y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the XGBoost parameters and perform cross validation to evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7762113793685632\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84      6302\n",
      "           1       0.83      0.69      0.75      2861\n",
      "           2       0.81      0.81      0.81     29687\n",
      "           3       0.72      0.70      0.71     20085\n",
      "           4       0.26      0.02      0.04       265\n",
      "           5       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.78     59230\n",
      "   macro avg       0.57      0.52      0.53     59230\n",
      "weighted avg       0.77      0.78      0.77     59230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# D√©finir les param√®tres du mod√®le\n",
    "xgb_params = {\n",
    "    'max_depth': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 1000,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 6,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "# Initialiser et entra√Æner\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# √âvaluation\n",
    "y_pred = xgb.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final training on the global data and submission on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "data = pd.read_csv('train_processed.csv', index_col=0)\n",
    "data = data.fillna(0)\n",
    "X = data.iloc[:, :-1].drop(columns=['N,A', 'Snow', 'urban_type', 'geography_type', 'change_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4','change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3' ,'change_status_date4'])\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "data_test = pd.read_csv('test_processed.csv', index_col=0)\n",
    "data_test = data_test.fillna(0)\n",
    "X_test_p = data_test.iloc[:, :].drop(columns=['N,A', 'Snow', 'urban_type', 'geography_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4','change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3' ,'change_status_date4'])\n",
    "\n",
    "\n",
    "# R√©entra√Æner sur tout le jeu d‚Äôentra√Ænement\n",
    "xgb_final = XGBClassifier(**xgb_params)\n",
    "xgb_final.fit(X, Y)\n",
    "\n",
    "# Pr√©dire sur le test\n",
    "y_test_pred = xgb_final.predict(X_test_p)\n",
    "\n",
    "# Sauvegarder la soumission\n",
    "pred_df = pd.DataFrame(y_test_pred, columns=['change_type'])\n",
    "pred_df.index.name = 'Id'\n",
    "pred_df.to_csv(\"submission_LMTPE.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
